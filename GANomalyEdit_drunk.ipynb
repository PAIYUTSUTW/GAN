{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import,print_function,division\n",
    "from keras import layers\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from pylab import rcParams\n",
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os\n",
    "import PIL\n",
    "import imageio\n",
    "import glob\n",
    "import time \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_len = 210\n",
    "num_singal = 13\n",
    "seq_step = 1\n",
    "value = 0\n",
    "width = num_singal\n",
    "height = seq_len\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def driver(seq_len, seq_step, num_singal,value):\n",
    "    ##########################################################\n",
    "    train=np.load('dataD_vse_13_normaly.npy',allow_pickle=True)\n",
    "    ##########################################################\n",
    "#     trains = pd.read_csv(\"Driving Data.csv\")\n",
    "#     print('load')\n",
    "#     dataA,dataB,dataC,dataD,dataE,dataF,dataG,dataH,dataI,dataJ=DriverA(trains)\n",
    "#     train = dataD.rolling(seq_length).std()\n",
    "#     train = train.values\n",
    "#     train = train[seq_length-1:,:]\n",
    "#     train=train[2500:12000,:]\n",
    "    m, n = train.shape  # m=562387, n=35\n",
    "    # normalization\n",
    "    for i in range(n - 1):\n",
    "        # print('i=', i)\n",
    "        A = max(train[:, i])\n",
    "        # print('A=', A)\n",
    "        if A != 0:\n",
    "#             train[:,i] = (train[:,i]-train[:,i].mean())/(train[:,i].std())\n",
    "            train[:,i] = (train[:,i]-train[:,i].min())/(train[:,i].max()-train[:,i].min())\n",
    "#             train[:, i] /= max(train[:, i])\n",
    "#             # scale from -1 to 1\n",
    "#             train[:, i] = 2 * train[:, i] - 1\n",
    "        else:\n",
    "            train[:, i] = train[:, i]\n",
    "\n",
    "    samples = train[:, :]\n",
    "    labels = np.full(shape=(len(train)),fill_value=value)\n",
    "    #labels = train[:, n - 1]  # the last colummn is label\n",
    "    #############################\n",
    "    ############################\n",
    "    # -- apply PCA dimension reduction for multi-variate GAN-AD -- #\n",
    "    from sklearn.decomposition import PCA\n",
    "    X_n = samples\n",
    "    ####################################\n",
    "    ###################################\n",
    "    # -- the best PC dimension is chosen pc=6 -- #\n",
    "    n_components = num_singal\n",
    "    pca = PCA(n_components)\n",
    "    pca.fit(X_n)\n",
    "    T_n = pca.transform(X_n)\n",
    "    ex_var = pca.explained_variance_ratio_\n",
    "    pc = pca.components_\n",
    "    # projected values on the principal component\n",
    "    T_n = np.matmul(X_n, pc.transpose(1, 0))\n",
    "    samples = T_n\n",
    "    # only for one-dimensional\n",
    "#     samples = T_n.reshape([samples.shape[0], ])\n",
    "    ###########################################\n",
    "    ###########################################\n",
    "    num_samples = (samples.shape[0] - seq_len) // seq_step\n",
    "    aa = np.empty([num_samples, seq_len, num_singal])\n",
    "    bb = np.empty([num_samples, seq_len, 1])\n",
    "    #print(samples)\n",
    "    #print(labels.shape)\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        bb[j, :, :] = np.reshape(labels[(j * seq_step):(j * seq_step + seq_len)], [-1, 1])\n",
    "        for i in range(num_singal):\n",
    "            aa[j, :, i] = samples[(j * seq_step):(j * seq_step + seq_len), i]\n",
    "\n",
    "    samples = aa\n",
    "#     samples = np.reshape(samples,[samples.shape[0],samples.shape[1],samples.shape[2],1])\n",
    "    labels = bb\n",
    "\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataD_train,labelD = driver(seq_len, seq_step, num_singal,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.26597373],\n",
       "         [-0.66034503],\n",
       "         [ 0.12964919],\n",
       "         ...,\n",
       "         [ 0.18701565],\n",
       "         [-0.01660965],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.26717952],\n",
       "         [-0.66007957],\n",
       "         [ 0.13169603],\n",
       "         ...,\n",
       "         [-0.39908193],\n",
       "         [-0.01637679],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.26681865],\n",
       "         [-0.66030773],\n",
       "         [ 0.12127676],\n",
       "         ...,\n",
       "         [-0.39889705],\n",
       "         [-0.01374262],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.6750223 ],\n",
       "         [-0.98212973],\n",
       "         [ 0.18015391],\n",
       "         ...,\n",
       "         [-0.27079815],\n",
       "         [-0.01785247],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.67502241],\n",
       "         [-0.98212944],\n",
       "         [ 0.18015357],\n",
       "         ...,\n",
       "         [-0.27076376],\n",
       "         [-0.01785249],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.67502241],\n",
       "         [-0.98212944],\n",
       "         [ 0.18015357],\n",
       "         ...,\n",
       "         [-0.27076376],\n",
       "         [-0.01785249],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.26717952],\n",
       "         [-0.66007957],\n",
       "         [ 0.13169603],\n",
       "         ...,\n",
       "         [-0.39908193],\n",
       "         [-0.01637679],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.26681865],\n",
       "         [-0.66030773],\n",
       "         [ 0.12127676],\n",
       "         ...,\n",
       "         [-0.39889705],\n",
       "         [-0.01374262],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.67797688],\n",
       "         [-0.97806758],\n",
       "         [ 0.16003694],\n",
       "         ...,\n",
       "         [-0.40179609],\n",
       "         [-0.01435853],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.67502241],\n",
       "         [-0.98212944],\n",
       "         [ 0.18015357],\n",
       "         ...,\n",
       "         [-0.27076376],\n",
       "         [-0.01785249],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.67502241],\n",
       "         [-0.98212944],\n",
       "         [ 0.18015357],\n",
       "         ...,\n",
       "         [-0.27076376],\n",
       "         [-0.01785249],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.6750223 ],\n",
       "         [-0.98212973],\n",
       "         [ 0.18015391],\n",
       "         ...,\n",
       "         [-0.27079815],\n",
       "         [-0.01785247],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.26681865],\n",
       "         [-0.66030773],\n",
       "         [ 0.12127676],\n",
       "         ...,\n",
       "         [-0.39889705],\n",
       "         [-0.01374262],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.67797688],\n",
       "         [-0.97806758],\n",
       "         [ 0.16003694],\n",
       "         ...,\n",
       "         [-0.40179609],\n",
       "         [-0.01435853],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.67797688],\n",
       "         [-0.97806758],\n",
       "         [ 0.16003694],\n",
       "         ...,\n",
       "         [-0.40179609],\n",
       "         [-0.01435853],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.67502241],\n",
       "         [-0.98212944],\n",
       "         [ 0.18015357],\n",
       "         ...,\n",
       "         [-0.27076376],\n",
       "         [-0.01785249],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.6750223 ],\n",
       "         [-0.98212973],\n",
       "         [ 0.18015391],\n",
       "         ...,\n",
       "         [-0.27079815],\n",
       "         [-0.01785247],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[-0.6750223 ],\n",
       "         [-0.98212973],\n",
       "         [ 0.18015391],\n",
       "         ...,\n",
       "         [-0.27079815],\n",
       "         [-0.01785247],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataD_train=dataD[np.random.choice(dataD.shape[0], dataD.shape[0]//2, replace=True), :,:]\n",
    "dataD_train = np.reshape(dataD_train,[dataD_train.shape[0],dataD_train.shape[1],dataD_train.shape[2],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_test():    \n",
    "    dataD = np.load('dataD_vse_13_normaly.npy',allow_pickle=True)\n",
    "    dataD_1 = dataD[0:1500,:]\n",
    "    dataD_2 = dataD[1500:3000,:]\n",
    "    dataD_3 = dataD[3000:6000,:]\n",
    "    dataD_4 = dataD[6000:7500,:]\n",
    "    dataD_5 = dataD[10000:,:]\n",
    "    dataD_normal=np.vstack((dataD_1,dataD_3))\n",
    "    dataD_normal=np.vstack((dataD_normal,dataD_5))\n",
    "    dataD_anormaly=np.vstack((dataD_2,dataD_4))\n",
    "    for i in range (len(dataD_anormaly)):\n",
    "        dataD_anormaly[i,0] = dataD_anormaly[i,0]*1.1\n",
    "        dataD_anormaly[i,1] = dataD_anormaly[i,1]*2.5\n",
    "        dataD_anormaly[i,2] = dataD_anormaly[i,2]*1.1\n",
    "    dataD_contrib = np.vstack((dataD_normal,dataD_anormaly))\n",
    "    label_0 = np.full(shape=(len(dataD_normal)),fill_value=0)\n",
    "    label_1 = np.full(shape=(len(dataD_anormaly)),fill_value=1)\n",
    "    m, n = dataD_contrib.shape\n",
    "    for i in range(n):\n",
    "        B = max(dataD_contrib[:, i])\n",
    "        if B != 0:\n",
    "    #         dataD_contrib[:,i] = (dataD_contrib[:,i]-dataD_contrib[:,i].mean())/(dataD_contrib[:,i].std())\n",
    "            dataD_contrib[:,i] = (dataD_contrib[:,i]-dataD_contrib[:,i].min())/(dataD_contrib[:,i].max()-dataD_contrib[:,i].min())\n",
    "    #         dataD_contrib[:, i] /= max(dataD_contrib[:, i])\n",
    "    #             # scale from -1 to 1\n",
    "    #         dataD_contrib[:, i] = 2 * dataD_contrib[:, i] - 1\n",
    "        else:\n",
    "            dataD_contrib[:, i] = dataD_contrib[:, i]\n",
    "    dataD_normal = dataD_contrib[:dataD_normal.shape[0],:]\n",
    "    dataD_anormaly = dataD_contrib[dataD_normal.shape[0]:,:]\n",
    "\n",
    "    num_samples_0 = (dataD_normal.shape[0] - seq_len) // 1\n",
    "    aa_0 = np.empty([num_samples_0, seq_len, num_singal])\n",
    "    bb_0 = np.empty([num_samples_0, seq_len, 1])\n",
    "    bbb_0 = np.empty([num_samples_0, seq_len, 1])\n",
    "    for j in range(num_samples_0):\n",
    "        bb_0[j, :, :] = np.reshape(label_0[(j * 1):(j * 1 + seq_len)], [-1, 1])\n",
    "    #     bbb_0[j, :, :] = np.reshape(idx_0[(j * 1):(j * 1 + 120)], [-1, 1])\n",
    "        for i in range(10):\n",
    "            aa_0[j, :, i] = dataD_normal[(j * 1):(j * 1 + seq_len), i]\n",
    "\n",
    "    num_samples_1 = (dataD_anormaly.shape[0] - seq_len) // 1\n",
    "    aa_1 = np.empty([num_samples_1, seq_len, num_singal])\n",
    "    bb_1 = np.empty([num_samples_1, seq_len, 1])\n",
    "    bbb_1 = np.empty([num_samples_1, seq_len, 1])\n",
    "    for j in range(num_samples_1):\n",
    "        bb_1[j, :, :] = np.reshape(label_1[(j * 1):(j * 1 + seq_len)], [-1, 1])\n",
    "    #     bbb_1[j, :, :] = np.reshape(idx_1[(j * 1):(j * 1 + 10)], [-1, 1])\n",
    "        for i in range(num_singal):\n",
    "            aa_1[j, :, i] = dataD_anormaly[(j * 1):(j * 1 + seq_len), i]\n",
    "    aa_1_A = aa_1[:1440,:]\n",
    "    aa_1_B = aa_1[1440:,:]\n",
    "    bb_1_A = bb_1[:1440,:]\n",
    "    bb_1_B = bb_1[1440:,:]\n",
    "    dataD_test = np.insert(aa_0,1500,aa_1_A,axis=0)\n",
    "    dataD_test = np.insert(dataD_test,5880,aa_1_B,axis=0)\n",
    "    labelD_test = np.insert(bb_0,1500,bb_1_A,axis=0)\n",
    "    labelD_test = np.insert(labelD_test,5880,bb_1_B,axis=0)\n",
    "    labelD_test = np.reshape(labelD_test,[labelD_test.shape[0],labelD_test.shape[1]])\n",
    "    labelD_test = np.sum(labelD_test,1)\n",
    "    labelD_test[labelD_test > 1] = 1\n",
    "        \n",
    "    index = [i for i in range(len(dataD_test))]\n",
    "    random.shuffle(index)\n",
    "    dataD_test = dataD_test[index]\n",
    "    labelD_test = labelD_test[index]\n",
    "    np.save(\"./score/labelD_test_drunk_\"+str(height)+\".npy\", labelD_test)\n",
    "    \n",
    "    dataD_test = np.reshape(dataD_test,[dataD_test.shape[0],dataD_test.shape[1],dataD_test.shape[2],1])\n",
    "    return dataD_test,labelD_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataD_test,labelD_test = driver_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10323, 210, 13, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataD_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 210, 13, 1)]      0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, 210, 13, 32)       832       \n",
      "_________________________________________________________________\n",
      "leaky_1 (LeakyReLU)          (None, 210, 13, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 105, 7, 64)        18496     \n",
      "_________________________________________________________________\n",
      "norm_1 (BatchNormalization)  (None, 105, 7, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_2 (LeakyReLU)          (None, 105, 7, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 53, 4, 128)        73856     \n",
      "_________________________________________________________________\n",
      "norm_2 (BatchNormalization)  (None, 53, 4, 128)        512       \n",
      "_________________________________________________________________\n",
      "leaky_3 (LeakyReLU)          (None, 53, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 27, 2, 128)        147584    \n",
      "_________________________________________________________________\n",
      "norm_3 (BatchNormalization)  (None, 27, 2, 128)        512       \n",
      "_________________________________________________________________\n",
      "leaky_4 (LeakyReLU)          (None, 27, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 14, 1, 256)        295168    \n",
      "_________________________________________________________________\n",
      "norm_4 (BatchNormalization)  (None, 14, 1, 256)        1024      \n",
      "_________________________________________________________________\n",
      "leaky_5 (LeakyReLU)          (None, 14, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "g_encoder_output (GlobalAver (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 538,240\n",
      "Trainable params: 537,088\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Generators Encoder\n",
    "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(32, (5,5), strides=(1,1), padding='same', name='conv_1', kernel_regularizer = 'l2')(input_layer)\n",
    "x = layers.LeakyReLU(name='leaky_1')(x)\n",
    "\n",
    "x = layers.Conv2D(64, (3,3), strides=(2,2), padding='same', name='conv_2', kernel_regularizer = 'l2')(x)\n",
    "x = layers.BatchNormalization(name='norm_1')(x)\n",
    "x = layers.LeakyReLU(name='leaky_2')(x)\n",
    "\n",
    "\n",
    "x = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='conv_3', kernel_regularizer = 'l2')(x)\n",
    "x = layers.BatchNormalization(name='norm_2')(x)\n",
    "x = layers.LeakyReLU(name='leaky_3')(x)\n",
    "\n",
    "\n",
    "x = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='conv_4', kernel_regularizer = 'l2')(x)\n",
    "x = layers.BatchNormalization(name='norm_3')(x)\n",
    "x = layers.LeakyReLU(name='leaky_4')(x)\n",
    "\n",
    "x = layers.Conv2D(256, (3,3), strides=(2,2), padding='same', name='conv_5', kernel_regularizer = 'l2')(x)\n",
    "x = layers.BatchNormalization(name='norm_4')(x)\n",
    "x = layers.LeakyReLU(name='leaky_5')(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D(name='g_encoder_output')(x)\n",
    "\n",
    "g_e = keras.models.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "g_e.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 210, 13, 1)]      0         \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, 256)               538240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2730)              701610    \n",
      "_________________________________________________________________\n",
      "de_reshape (Reshape)         (None, 210, 13, 1)        0         \n",
      "_________________________________________________________________\n",
      "deconv_1 (Conv2DTranspose)   (None, 210, 13, 128)      1280      \n",
      "_________________________________________________________________\n",
      "de_leaky_1 (LeakyReLU)       (None, 210, 13, 128)      0         \n",
      "_________________________________________________________________\n",
      "deconv_2 (Conv2DTranspose)   (None, 210, 13, 64)       73792     \n",
      "_________________________________________________________________\n",
      "de_leaky_2 (LeakyReLU)       (None, 210, 13, 64)       0         \n",
      "_________________________________________________________________\n",
      "deconv_3 (Conv2DTranspose)   (None, 210, 13, 32)       18464     \n",
      "_________________________________________________________________\n",
      "de_leaky_3 (LeakyReLU)       (None, 210, 13, 32)       0         \n",
      "_________________________________________________________________\n",
      "decoder_deconv_output (Conv2 (None, 210, 13, 1)        289       \n",
      "=================================================================\n",
      "Total params: 1,333,675\n",
      "Trainable params: 1,332,523\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Generator\n",
    "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
    "\n",
    "x = g_e(input_layer)\n",
    "\n",
    "y = layers.Dense(height*width, name='dense')(x) # 2 = 128 / 8 / 8\n",
    "y = layers.Reshape((height, width,1), name='de_reshape')(y)\n",
    "\n",
    "# y = layers.Conv2DTranspose(256, (3,3), strides=(1,1), padding='valid', name='deconv_0', kernel_regularizer = 'l2')(y)\n",
    "# y = layers.LeakyReLU(name='de_leaky_0')(y)\n",
    "\n",
    "y = layers.Conv2DTranspose(128, (3,3), strides=(1,1), padding='same', name='deconv_1', kernel_regularizer = 'l2')(y)\n",
    "y = layers.LeakyReLU(name='de_leaky_1')(y)\n",
    "\n",
    "y = layers.Conv2DTranspose(64, (3,3), strides=(1,1), padding='same', name='deconv_2', kernel_regularizer = 'l2')(y)\n",
    "y = layers.LeakyReLU(name='de_leaky_2')(y)\n",
    "\n",
    "y = layers.Conv2DTranspose(32, (3,3), strides=(1,1), padding='same', name='deconv_3', kernel_regularizer = 'l2')(y)\n",
    "y = layers.LeakyReLU(name='de_leaky_3')(y)\n",
    "\n",
    "y = layers.Conv2DTranspose(channels, (3, 3), strides=(1,1), padding='same', name='decoder_deconv_output', kernel_regularizer = 'l2', activation='tanh')(y)\n",
    "\n",
    "g = keras.models.Model(inputs=input_layer, outputs=y)\n",
    "\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 210, 13, 1)]      0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)      (None, 210, 13, 32)       832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 210, 13, 32)       0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)      (None, 105, 7, 64)        18496     \n",
      "_________________________________________________________________\n",
      "encoder_norm_1 (BatchNormali (None, 105, 7, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 105, 7, 64)        0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)      (None, 53, 4, 128)        73856     \n",
      "_________________________________________________________________\n",
      "encoder_norm_2 (BatchNormali (None, 53, 4, 128)        512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 53, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_4 (Conv2D)      (None, 27, 2, 256)        295168    \n",
      "_________________________________________________________________\n",
      "encoder_norm_3 (BatchNormali (None, 27, 2, 256)        1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 27, 2, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv_41 (Conv2D)             (None, 14, 1, 256)        590080    \n",
      "_________________________________________________________________\n",
      "encoder_norm_4 (BatchNormali (None, 14, 1, 256)        1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 14, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "encoder_output (GlobalAverag (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 981,248\n",
      "Trainable params: 979,840\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Encoder\n",
    "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
    "\n",
    "z = layers.Conv2D(32, (5,5), strides=(1,1), padding='same', name='encoder_conv_1', kernel_regularizer = 'l2')(input_layer)\n",
    "z = layers.LeakyReLU()(z)\n",
    "\n",
    "z = layers.Conv2D(64, (3,3), strides=(2,2), padding='same', name='encoder_conv_2', kernel_regularizer = 'l2')(z)\n",
    "z = layers.BatchNormalization(name='encoder_norm_1')(z)\n",
    "z = layers.LeakyReLU()(z)\n",
    "\n",
    "\n",
    "z = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='encoder_conv_3', kernel_regularizer = 'l2')(z)\n",
    "z = layers.BatchNormalization(name='encoder_norm_2')(z)\n",
    "z = layers.LeakyReLU()(z)\n",
    "\n",
    "z = layers.Conv2D(256, (3,3), strides=(2,2), padding='same', name='encoder_conv_4', kernel_regularizer = 'l2')(z)\n",
    "z = layers.BatchNormalization(name='encoder_norm_3')(z)\n",
    "z = layers.LeakyReLU()(z)\n",
    "\n",
    "\n",
    "z = layers.Conv2D(256, (3,3), strides=(2,2), padding='same', name='conv_41', kernel_regularizer = 'l2')(z)\n",
    "z = layers.BatchNormalization(name='encoder_norm_4')(z)\n",
    "z = layers.LeakyReLU()(z)\n",
    "\n",
    "z = layers.GlobalAveragePooling2D(name='encoder_output')(z)\n",
    "\n",
    "encoder = keras.models.Model(input_layer, z)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 210, 13, 1)]      0         \n",
      "_________________________________________________________________\n",
      "f_conv_1 (Conv2D)            (None, 210, 13, 32)       832       \n",
      "_________________________________________________________________\n",
      "f_leaky_1 (LeakyReLU)        (None, 210, 13, 32)       0         \n",
      "_________________________________________________________________\n",
      "f_conv_2 (Conv2D)            (None, 105, 7, 64)        18496     \n",
      "_________________________________________________________________\n",
      "f_norm_1 (BatchNormalization (None, 105, 7, 64)        256       \n",
      "_________________________________________________________________\n",
      "f_leaky_2 (LeakyReLU)        (None, 105, 7, 64)        0         \n",
      "_________________________________________________________________\n",
      "f_conv_3 (Conv2D)            (None, 53, 4, 128)        73856     \n",
      "_________________________________________________________________\n",
      "f_norm_2 (BatchNormalization (None, 53, 4, 128)        512       \n",
      "_________________________________________________________________\n",
      "f_leaky_3 (LeakyReLU)        (None, 53, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "f_conv_4 (Conv2D)            (None, 27, 2, 256)        295168    \n",
      "_________________________________________________________________\n",
      "f_norm_3 (BatchNormalization (None, 27, 2, 256)        1024      \n",
      "_________________________________________________________________\n",
      "f_leaky_4 (LeakyReLU)        (None, 27, 2, 256)        0         \n",
      "_________________________________________________________________\n",
      "f_conv_5 (Conv2D)            (None, 14, 1, 256)        590080    \n",
      "_________________________________________________________________\n",
      "f_norm_4 (BatchNormalization (None, 14, 1, 256)        1024      \n",
      "_________________________________________________________________\n",
      "feature_output (LeakyReLU)   (None, 14, 1, 256)        0         \n",
      "=================================================================\n",
      "Total params: 981,248\n",
      "Trainable params: 979,840\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#feature extractor\n",
    "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
    "\n",
    "f = layers.Conv2D(32, (5,5), strides=(1,1), padding='same', name='f_conv_1', kernel_regularizer = 'l2')(input_layer)\n",
    "f = layers.LeakyReLU(name='f_leaky_1')(f)\n",
    "\n",
    "f = layers.Conv2D(64, (3,3), strides=(2,2), padding='same', name='f_conv_2', kernel_regularizer = 'l2')(f)\n",
    "f = layers.BatchNormalization(name='f_norm_1')(f)\n",
    "f = layers.LeakyReLU(name='f_leaky_2')(f)\n",
    "\n",
    "\n",
    "f = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='f_conv_3', kernel_regularizer = 'l2')(f)\n",
    "f = layers.BatchNormalization(name='f_norm_2')(f)\n",
    "f = layers.LeakyReLU(name='f_leaky_3')(f)\n",
    "\n",
    "f = layers.Conv2D(256, (3,3), strides=(2,2), padding='same', name='f_conv_4', kernel_regularizer = 'l2')(f)\n",
    "f = layers.BatchNormalization(name='f_norm_3')(f)\n",
    "f = layers.LeakyReLU(name='f_leaky_4')(f)\n",
    "\n",
    "f = layers.Conv2D(256, (3,3), strides=(2,2), padding='same', name='f_conv_5', kernel_regularizer = 'l2')(f)\n",
    "f = layers.BatchNormalization(name='f_norm_4')(f)\n",
    "f = layers.LeakyReLU(name='feature_output')(f)\n",
    "\n",
    "feature_extractor = keras.models.Model(input_layer, f)\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gan trainer\n",
    "class AdvLoss(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AdvLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ori_feature = feature_extractor(x[0])\n",
    "        gan_feature = feature_extractor(x[1])\n",
    "        return K.mean(K.square(ori_feature - K.mean(gan_feature, axis=0)))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "    \n",
    "class CntLoss(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CntLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "        return K.mean(K.abs(ori - gan))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "    \n",
    "class EncLoss(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(EncLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "        return K.mean(K.square(g_e(ori) - encoder(gan)))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "    \n",
    "# model for training\n",
    "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
    "gan = g(input_layer) # g(x)\n",
    "\n",
    "adv_loss = AdvLoss(name='adv_loss')([input_layer, gan])\n",
    "cnt_loss = CntLoss(name='cnt_loss')([input_layer, gan])\n",
    "enc_loss = EncLoss(name='enc_loss')([input_layer, gan])\n",
    "\n",
    "gan_trainer = keras.models.Model(input_layer, [adv_loss, cnt_loss, enc_loss])\n",
    "\n",
    "# loss function\n",
    "def loss(yt, yp):\n",
    "    return yp\n",
    "\n",
    "losses = {\n",
    "    'adv_loss': loss,\n",
    "    'cnt_loss': loss,\n",
    "    'enc_loss': loss,\n",
    "}\n",
    "\n",
    "lossWeights = {'cnt_loss': 20.0, 'adv_loss': 1.0, 'enc_loss': 1.0}\n",
    "\n",
    "# compile\n",
    "gan_trainer.compile(optimizer = 'adam', loss=losses, loss_weights=lossWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 210, 13, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 210, 13, 1)   1333675     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "adv_loss (AdvLoss)              ()                   0           input[0][0]                      \n",
      "                                                                 model_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cnt_loss (CntLoss)              ()                   0           input[0][0]                      \n",
      "                                                                 model_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "enc_loss (EncLoss)              ()                   0           input[0][0]                      \n",
      "                                                                 model_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,333,675\n",
      "Trainable params: 1,332,523\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_trainer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 210, 13, 1)]      0         \n",
      "_________________________________________________________________\n",
      "model_3 (Functional)         (None, 14, 1, 256)        981248    \n",
      "_________________________________________________________________\n",
      "glb_avg (GlobalAveragePoolin (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "d_out (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 981,505\n",
      "Trainable params: 980,097\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#discriminator\n",
    "\n",
    "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
    "\n",
    "f = feature_extractor(input_layer)\n",
    "\n",
    "d = layers.GlobalAveragePooling2D(name='glb_avg')(f)\n",
    "d = layers.Dense(1, activation='sigmoid', name='d_out')(d)\n",
    "    \n",
    "d = keras.models.Model(input_layer, d)\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Evaluation(data,label,epoch):\n",
    "    #Evaluation\n",
    "    encoded = g_e.predict(data)\n",
    "    gan_x = g.predict(data)\n",
    "    encoded_gan = g_e.predict(gan_x)\n",
    "    score = np.sum(np.absolute(encoded - encoded_gan), axis=-1)\n",
    "    score = (score - np.min(score)) / (np.max(score) - np.min(score)) # map to 0~1\n",
    "    np.save(\"./score/driver_drunk\"+str(epoch)+\"_\"+str(height)+\".npy\", score)\n",
    "#     D_L = np.empty([score.shape[0], ])\n",
    "#     for i in range(score.shape[0]):\n",
    "#         if score[i]>np.percentile(score, 75):\n",
    "#             # true/negative\n",
    "#             D_L[i] = 1\n",
    "#         else:\n",
    "#             # false/positive\n",
    "#             D_L[i] = 0\n",
    "    # fpr, tpr, thresholds = metrics.roc_curve(labelAF, D_L, pos_label = 0)\n",
    "    # Auc = metrics.auc(fpr, tpr)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labelD_test, D_L, average='binary')        \n",
    "    rcParams['figure.figsize'] = 14, 5\n",
    "    plt.scatter(range(len(score)),score, c=['skyblue' if x == 0 else 'pink' for x in label])\n",
    "    plt.savefig(\"./plot/driver_drunk\"+str(epoch)+\"_\"+str(height)+\".png\")\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return #precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "niter = 50000\n",
    "bz = 32\n",
    "def get_data_generator(data, batch_size=32):\n",
    "    datalen = len(data)\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        idxes = np.arange(datalen)\n",
    "        np.random.shuffle(idxes)\n",
    "        cnt += 1\n",
    "        for i in range(int(np.ceil(datalen/batch_size))):\n",
    "            train_x = np.take(data, idxes[i*batch_size: (i+1) * batch_size], axis=0)\n",
    "            y = np.ones(len(train_x))\n",
    "            yield train_x, [y, y, y]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_generator = get_data_generator(dataD_train, bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niter: 1, g_loss: [5.601454257965088, 4.170488491898455e-22, 0.0, 0.0], d_loss: 6.256426811218262\n",
      "\n",
      "niter: 301, g_loss: [0.0898471549153328, 9.572007542146821e-08, 9.750744357006624e-05, 0.0007714746752753854], d_loss: 1.4616799354553223\n",
      "\n",
      "niter: 601, g_loss: [0.019320862367749214, 9.73754026745155e-07, 0.00015121157048270106, 0.00028387096244841814], d_loss: 0.37496283650398254\n",
      "\n",
      "niter: 901, g_loss: [0.5677976608276367, 0.07996038347482681, 0.014587851241230965, 0.10808705538511276], d_loss: 0.5387366414070129\n",
      "\n",
      "niter: 1201, g_loss: [0.024473998695611954, 4.312503733672202e-06, 7.400968752335757e-05, 0.00040749908657744527], d_loss: 0.17704276740550995\n",
      "\n",
      "niter: 1501, g_loss: [0.013535511679947376, 7.473980758732068e-07, 2.52585050475318e-05, 0.0004293660749681294], d_loss: 1.1617894172668457\n",
      "\n",
      "niter: 1801, g_loss: [0.0035982169210910797, 5.3375799780042144e-08, 1.973814141820185e-05, 3.06607034872286e-05], d_loss: 0.8375533223152161\n",
      "\n",
      "niter: 2101, g_loss: [1.9044772386550903, 1.654741026868578e-05, 0.002703501842916012, 0.019551243633031845], d_loss: 0.41059640049934387\n",
      "\n",
      "niter: 2401, g_loss: [0.7866191864013672, 7.463314977940172e-05, 0.0017699351301416755, 0.009314543567597866], d_loss: 0.07872866094112396\n",
      "\n",
      "niter: 2701, g_loss: [15.036653518676758, 8.053729057312012, 0.12621788680553436, 0.048676446080207825], d_loss: 1.235931396484375\n",
      "\n",
      "niter: 3001, g_loss: [4.8361592292785645, 6.285604467848316e-05, 0.0031755699310451746, 0.05341387167572975], d_loss: 0.20918965339660645\n",
      "\n",
      "niter: 3301, g_loss: [4.158863544464111, 0.009703027084469795, 0.0021249386481940746, 0.05159232020378113], d_loss: 0.12297167629003525\n",
      "\n",
      "niter: 3601, g_loss: [4.164840221405029, 0.00021692758309654891, 0.0015686964616179466, 0.05331873521208763], d_loss: 0.14163216948509216\n",
      "\n",
      "niter: 3901, g_loss: [3.6071853637695312, 0.0009719048975966871, 0.0012592783896252513, 0.04434754326939583], d_loss: 0.13035453855991364\n",
      "\n",
      "niter: 4201, g_loss: [10.224556922912598, 0.03272103890776634, 0.10777999460697174, 0.07159952819347382], d_loss: 0.5840810537338257\n",
      "\n",
      "niter: 4501, g_loss: [8.101175308227539, 5.736931780120358e-05, 0.013878019526600838, 0.05848020687699318], d_loss: 0.4503157138824463\n",
      "\n",
      "niter: 4801, g_loss: [7.8994598388671875, 0.0003938529989682138, 0.013287406414747238, 0.0559527724981308], d_loss: 0.07295030355453491\n",
      "\n",
      "niter: 5101, g_loss: [7.710446357727051, 0.019695142284035683, 0.012184304185211658, 0.058440253138542175], d_loss: 0.0840771347284317\n",
      "\n",
      "niter: 5401, g_loss: [7.572127819061279, 1.8345017451792955e-05, 0.016179034486413002, 0.06142511963844299], d_loss: 0.1302698254585266\n",
      "\n",
      "niter: 5701, g_loss: [7.211986064910889, 0.0005311267450451851, 0.009238798171281815, 0.050155214965343475], d_loss: 0.7122986316680908\n",
      "\n",
      "niter: 6001, g_loss: [6.981750011444092, 0.0007698507979512215, 0.008693271316587925, 0.046929843723773956], d_loss: 0.133791983127594\n",
      "\n",
      "niter: 6301, g_loss: [6.750798225402832, 0.0028514324221760035, 0.008297426626086235, 0.04578685015439987], d_loss: 0.04051690548658371\n",
      "\n",
      "niter: 6601, g_loss: [8.246708869934082, 0.0027445165906101465, 0.036514222621917725, 0.07158468663692474], d_loss: 0.14103633165359497\n",
      "\n",
      "niter: 6901, g_loss: [7.864943027496338, 1.8055736745736795e-06, 0.03044009394943714, 0.0700036883354187], d_loss: 0.02518639899790287\n",
      "\n",
      "niter: 7201, g_loss: [7.52533483505249, 7.464680493285414e-06, 0.026761546730995178, 0.0691247507929802], d_loss: 0.008203089237213135\n",
      "\n",
      "niter: 7501, g_loss: [7.189077854156494, 1.1334709597576875e-05, 0.023735113441944122, 0.06825125962495804], d_loss: 0.004316064063459635\n",
      "\n",
      "niter: 7801, g_loss: [13.239773750305176, 0.004379768390208483, 0.10295029729604721, 0.07887078076601028], d_loss: 0.6928982734680176\n",
      "\n",
      "niter: 8101, g_loss: [13.034064292907715, 0.01833144761621952, 0.09408615529537201, 0.07963277399539948], d_loss: 0.2749110162258148\n",
      "\n",
      "niter: 8401, g_loss: [12.822227478027344, 0.024490727111697197, 0.08560815453529358, 0.0799819752573967], d_loss: 0.12721295654773712\n",
      "\n",
      "niter: 8701, g_loss: [13.739015579223633, 1.1381474733352661, 0.07827401906251907, 0.07924489676952362], d_loss: 0.13108333945274353\n",
      "\n",
      "niter: 9001, g_loss: [11.928857803344727, 0.006551895756274462, 0.04753061383962631, 0.07724463939666748], d_loss: 0.24171240627765656\n",
      "\n",
      "niter: 9301, g_loss: [12.048778533935547, 0.3115970194339752, 0.04175777733325958, 0.07698994874954224], d_loss: 0.27306249737739563\n",
      "\n",
      "niter: 9601, g_loss: [11.562631607055664, 0.0004861532070208341, 0.03694390505552292, 0.07723452895879745], d_loss: 0.02341403253376484\n",
      "\n",
      "niter: 9901, g_loss: [11.485464096069336, 0.006022431887686253, 0.034604135900735855, 0.07778222858905792], d_loss: 0.2104904055595398\n",
      "\n",
      "niter: 10201, g_loss: [11.18320369720459, 0.005006766878068447, 0.024653974920511246, 0.07703670859336853], d_loss: 0.07403548061847687\n",
      "\n",
      "niter: 10501, g_loss: [10.97970962524414, 0.004795091692358255, 0.02003735490143299, 0.07594898343086243], d_loss: 0.02940032258629799\n",
      "\n",
      "niter: 10801, g_loss: [11.162887573242188, 0.002510738791897893, 0.019847625866532326, 0.07337921112775803], d_loss: 0.3020089864730835\n",
      "\n",
      "niter: 11101, g_loss: [10.996618270874023, 0.0032701955642551184, 0.018085749819874763, 0.07377000153064728], d_loss: 0.09297485649585724\n",
      "\n",
      "niter: 11401, g_loss: [10.820538520812988, 0.0028543914668262005, 0.016583213582634926, 0.07407630980014801], d_loss: 0.03878019377589226\n",
      "\n",
      "niter: 11701, g_loss: [12.27761459350586, 0.008090582676231861, 0.06813851743936539, 0.07783442735671997], d_loss: 0.1671793907880783\n",
      "\n",
      "niter: 12001, g_loss: [11.817150115966797, 0.003598874667659402, 0.0534152016043663, 0.07874824851751328], d_loss: 0.11986617743968964\n",
      "\n",
      "niter: 12301, g_loss: [11.577482223510742, 0.2512577176094055, 0.03796236962080002, 0.07943171262741089], d_loss: 0.06863471120595932\n",
      "\n",
      "niter: 12601, g_loss: [10.925010681152344, 0.0027942939195781946, 0.027597112581133842, 0.07854384183883667], d_loss: 0.005175785627216101\n",
      "\n",
      "niter: 12901, g_loss: [10.609319686889648, 0.015027353540062904, 0.021925389766693115, 0.07748185098171234], d_loss: 0.0022659117821604013\n",
      "\n",
      "niter: 13201, g_loss: [27.154163360595703, 0.10590987652540207, 0.6857563853263855, 0.07690118998289108], d_loss: 0.29285508394241333\n",
      "\n",
      "niter: 13501, g_loss: [31.954172134399414, 5.590880393981934, 0.6599414348602295, 0.07673558592796326], d_loss: 0.09942524135112762\n",
      "\n",
      "niter: 13801, g_loss: [25.665279388427734, 0.009974892251193523, 0.6337904334068298, 0.0762108564376831], d_loss: 0.024896031245589256\n",
      "\n",
      "niter: 14101, g_loss: [26.569133758544922, 0.07039853930473328, 0.6807863116264343, 0.07922546565532684], d_loss: 0.049679506570100784\n",
      "\n",
      "niter: 14401, g_loss: [33.49565505981445, 0.03149266168475151, 0.9822363257408142, 0.07227881997823715], d_loss: 0.334881067276001\n",
      "\n",
      "niter: 14701, g_loss: [33.180076599121094, 6.87891078996472e-05, 0.9774693250656128, 0.072178415954113], d_loss: 0.014321730472147465\n",
      "\n",
      "niter: 15001, g_loss: [32.784400939941406, 0.015442869625985622, 0.9673645496368408, 0.07264973223209381], d_loss: 0.005292209796607494\n",
      "\n",
      "niter: 15301, g_loss: [32.64904022216797, 0.2537899315357208, 0.9600244164466858, 0.07263588905334473], d_loss: 0.002896693767979741\n",
      "\n",
      "niter: 15601, g_loss: [32.65856170654297, 0.6445147395133972, 0.9531904458999634, 0.07259947061538696], d_loss: 0.001815919065847993\n",
      "\n",
      "niter: 15901, g_loss: [32.38661575317383, 0.757425844669342, 0.9470828175544739, 0.07256777584552765], d_loss: 0.0012205707607790828\n",
      "\n",
      "niter: 16201, g_loss: [31.958515167236328, 0.6991568803787231, 0.9426323175430298, 0.07240132987499237], d_loss: 0.0008606650517322123\n",
      "\n",
      "niter: 16501, g_loss: [31.45477294921875, 0.6590107679367065, 0.9343783855438232, 0.07216547429561615], d_loss: 0.0006317583029158413\n",
      "\n",
      "niter: 16801, g_loss: [30.301862716674805, 0.3701593577861786, 0.906890332698822, 0.07155827432870865], d_loss: 0.0004977805074304342\n",
      "\n",
      "niter: 17101, g_loss: [28.106468200683594, 0.3279465138912201, 0.8158165812492371, 0.07037030160427094], d_loss: 0.0003739831445273012\n",
      "\n",
      "niter: 17401, g_loss: [26.468128204345703, 0.1690906435251236, 0.7594233751296997, 0.07080449163913727], d_loss: 0.00029953825287520885\n",
      "\n",
      "niter: 17701, g_loss: [25.394306182861328, 0.22997355461120605, 0.7210995554924011, 0.07098378986120224], d_loss: 0.0002382235979894176\n",
      "\n",
      "niter: 18001, g_loss: [34.6732063293457, 0.15092751383781433, 0.9883420467376709, 0.06726952642202377], d_loss: 1.8534114360809326\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niter: 18301, g_loss: [34.528968811035156, 0.01456661056727171, 0.9879539608955383, 0.06730516254901886], d_loss: 0.7841127514839172\n",
      "\n",
      "niter: 18601, g_loss: [34.590248107910156, 0.0832817405462265, 0.9875906705856323, 0.06732773780822754], d_loss: 0.47549697756767273\n",
      "\n",
      "niter: 18901, g_loss: [34.76506805419922, 0.2624122202396393, 0.9873858690261841, 0.06734424084424973], d_loss: 0.3194562494754791\n",
      "\n",
      "niter: 19201, g_loss: [34.412906646728516, 0.07477857172489166, 0.9715421199798584, 0.21997341513633728], d_loss: 0.2261742204427719\n",
      "\n",
      "niter: 19501, g_loss: [34.85010528564453, 0.361408531665802, 0.9867147207260132, 0.06742341816425323], d_loss: 0.16569823026657104\n",
      "\n",
      "niter: 19801, g_loss: [34.6798095703125, 0.19618341326713562, 0.9864804148674011, 0.06744121760129929], d_loss: 0.12462341040372849\n",
      "\n",
      "niter: 20101, g_loss: [34.93398666381836, 0.45628827810287476, 0.986207127571106, 0.06745213270187378], d_loss: 0.09565497189760208\n",
      "\n",
      "niter: 20401, g_loss: [34.91082000732422, 0.43932491540908813, 0.9859253168106079, 0.0674523338675499], d_loss: 0.07461655139923096\n",
      "\n",
      "niter: 20701, g_loss: [35.02414321899414, 0.5615061521530151, 0.9855154156684875, 0.06745507568120956], d_loss: 0.05897087976336479\n",
      "\n",
      "niter: 21001, g_loss: [35.094017028808594, 0.6435835957527161, 0.9849424958229065, 0.06749089807271957], d_loss: 0.047101400792598724\n",
      "\n",
      "niter: 21301, g_loss: [35.121482849121094, 0.6782626509666443, 0.9846261143684387, 0.06753112375736237], d_loss: 0.037941452115774155\n",
      "\n",
      "niter: 21601, g_loss: [35.14957809448242, 0.7136939167976379, 0.9843129515647888, 0.06754560023546219], d_loss: 0.030770478770136833\n",
      "\n",
      "niter: 21901, g_loss: [35.117305755615234, 0.6902704238891602, 0.9839338660240173, 0.06755182147026062], d_loss: 0.02508458122611046\n",
      "\n",
      "niter: 22201, g_loss: [35.10991287231445, 0.6946229934692383, 0.9834209680557251, 0.06755060702562332], d_loss: 0.020516246557235718\n",
      "\n",
      "niter: 22501, g_loss: [35.08308410644531, 0.6764984130859375, 0.9830715656280518, 0.06755658984184265], d_loss: 0.016806527972221375\n",
      "\n",
      "niter: 22801, g_loss: [35.059852600097656, 0.6612873077392578, 0.9827703833580017, 0.06756699085235596], d_loss: 0.013758485205471516\n",
      "\n",
      "niter: 23101, g_loss: [35.05001449584961, 0.6603775024414062, 0.9824402928352356, 0.06758077442646027], d_loss: 0.011228427290916443\n",
      "\n",
      "niter: 23401, g_loss: [35.059059143066406, 0.6799130439758301, 0.9820512533187866, 0.06759661436080933], d_loss: 0.009109357371926308\n",
      "\n",
      "niter: 23701, g_loss: [35.05126953125, 0.6848087906837463, 0.9815739989280701, 0.06761139631271362], d_loss: 0.007323102559894323\n",
      "\n",
      "niter: 24001, g_loss: [35.037776947021484, 0.682183563709259, 0.9812136888504028, 0.06761176884174347], d_loss: 0.0058134738355875015\n",
      "\n",
      "niter: 24301, g_loss: [35.01404571533203, 0.6696232557296753, 0.9808670878410339, 0.06761832535266876], d_loss: 0.004540552385151386\n",
      "\n",
      "niter: 24601, g_loss: [35.01984786987305, 0.688787579536438, 0.980444610118866, 0.06762398779392242], d_loss: 0.00347712030634284\n",
      "\n",
      "niter: 24901, g_loss: [34.97261047363281, 0.6602404713630676, 0.979793906211853, 0.06761704385280609], d_loss: 0.0026029441505670547\n",
      "\n",
      "niter: 25201, g_loss: [34.51921844482422, 0.28520044684410095, 0.9687439203262329, 0.2168033868074417], d_loss: 0.001900726929306984\n",
      "\n",
      "niter: 25501, g_loss: [34.868751525878906, 0.612945556640625, 0.9776710867881775, 0.06759887933731079], d_loss: 0.0013512071454897523\n",
      "\n",
      "niter: 25801, g_loss: [34.82480239868164, 0.5993081331253052, 0.9765918254852295, 0.06759535521268845], d_loss: 0.0009372896747663617\n",
      "\n",
      "niter: 26101, g_loss: [34.71659469604492, 0.5478676557540894, 0.9742614030838013, 0.06752767413854599], d_loss: 0.0006392875220626593\n",
      "\n",
      "niter: 26401, g_loss: [34.58371353149414, 0.4980173110961914, 0.970703125, 0.06730113923549652], d_loss: 0.00043641592492349446\n",
      "\n",
      "niter: 26701, g_loss: [35.51102828979492, 0.30839258432388306, 0.993628740310669, 0.0761176198720932], d_loss: 0.49447718262672424\n",
      "\n",
      "niter: 27001, g_loss: [35.461666107177734, 0.22910244762897491, 0.9964264035224915, 0.0757112205028534], d_loss: 0.4445469379425049\n",
      "\n",
      "niter: 27301, g_loss: [35.168338775634766, 0.0024031305219978094, 0.9939336180686951, 0.07572469860315323], d_loss: 0.07784658670425415\n",
      "\n",
      "niter: 27601, g_loss: [35.14839553833008, 0.03066563606262207, 0.9924909472465515, 0.07574211061000824], d_loss: 0.0404757522046566\n",
      "\n",
      "niter: 27901, g_loss: [35.225685119628906, 0.14419054985046387, 0.9917967915534973, 0.07575473189353943], d_loss: 0.02391594834625721\n",
      "\n",
      "niter: 28201, g_loss: [35.15959930419922, 0.09165457636117935, 0.9923984408378601, 0.07573588192462921], d_loss: 0.014906076714396477\n",
      "\n",
      "niter: 28501, g_loss: [35.23735809326172, 0.1573948860168457, 0.9943743944168091, 0.07549679279327393], d_loss: 0.01636394113302231\n",
      "\n",
      "niter: 28801, g_loss: [35.03935623168945, 0.0006057210266590118, 0.9937838912010193, 0.07478656619787216], d_loss: 0.009169788099825382\n",
      "\n",
      "niter: 29101, g_loss: [34.95484924316406, 0.00669712433591485, 0.9911812543869019, 0.07493747770786285], d_loss: 0.004211902152746916\n",
      "\n",
      "niter: 29401, g_loss: [35.127410888671875, 0.25116002559661865, 0.9897979497909546, 0.07502643018960953], d_loss: 0.002630881266668439\n",
      "\n",
      "niter: 29701, g_loss: [35.427310943603516, 0.5904024839401245, 0.9867390394210815, 0.14700451493263245], d_loss: 0.001694852253422141\n",
      "\n",
      "niter: 30001, g_loss: [35.49916076660156, 0.7515314817428589, 0.9887365102767944, 0.07497045397758484], d_loss: 0.0010712145594879985\n",
      "\n",
      "niter: 30301, g_loss: [35.34331130981445, 0.6829794645309448, 0.9876284003257751, 0.07500292360782623], d_loss: 0.0007073857705108821\n",
      "\n",
      "niter: 30601, g_loss: [66.78653717041016, 32.21094512939453, 0.9870971441268921, 0.07514246553182602], d_loss: 0.003391854465007782\n",
      "\n",
      "niter: 30901, g_loss: [35.18355941772461, 0.6359103918075562, 0.9898772239685059, 0.07492206245660782], d_loss: 0.0003763926506508142\n",
      "\n",
      "niter: 31201, g_loss: [35.094505310058594, 0.67420893907547, 0.9882484078407288, 0.07480438798666], d_loss: 0.0002888374147005379\n",
      "\n",
      "niter: 31501, g_loss: [34.96219253540039, 0.6782647371292114, 0.9867501258850098, 0.07490427047014236], d_loss: 0.00025280460249632597\n",
      "\n",
      "niter: 31801, g_loss: [34.81029510498047, 0.6646784543991089, 0.9858410954475403, 0.07489177584648132], d_loss: 0.00021110777743160725\n",
      "\n",
      "niter: 32101, g_loss: [34.54779052734375, 0.5521997809410095, 0.9850534200668335, 0.07521068304777145], d_loss: 0.0002001043176278472\n",
      "\n",
      "niter: 32401, g_loss: [34.432533264160156, 0.620997965335846, 0.9833784103393555, 0.07534454762935638], d_loss: 0.00018253893358632922\n",
      "\n",
      "niter: 32701, g_loss: [36.83342742919922, 3.19872784614563, 0.9829005002975464, 0.07528677582740784], d_loss: 0.0005085485754534602\n",
      "\n",
      "niter: 33001, g_loss: [33.94934844970703, 0.5360214114189148, 0.9811758399009705, 0.0753692239522934], d_loss: 0.00016428552044089884\n",
      "\n",
      "niter: 33301, g_loss: [33.75282669067383, 0.5618162155151367, 0.9804585576057434, 0.07548275589942932], d_loss: 0.00015934824477881193\n",
      "\n",
      "niter: 33601, g_loss: [33.21077346801758, 0.13870270550251007, 0.9861025214195251, 0.07549680024385452], d_loss: 0.00019116232579108328\n",
      "\n",
      "niter: 33901, g_loss: [33.473690032958984, 0.6562204360961914, 0.9860125184059143, 0.07562053203582764], d_loss: 0.00028956684400327504\n",
      "\n",
      "niter: 34201, g_loss: [32.69340515136719, 0.3146476745605469, 0.9782536029815674, 0.07627283781766891], d_loss: 0.00013124138058628887\n",
      "\n",
      "niter: 34501, g_loss: [33.848331451416016, 1.8546303510665894, 0.9739904999732971, 0.07595104724168777], d_loss: 0.00028349243802949786\n",
      "\n",
      "niter: 34801, g_loss: [42.876712799072266, 1.7772481441497803, 0.9978830218315125, 0.08155499398708344], d_loss: 2.318723201751709\n",
      "\n",
      "niter: 35101, g_loss: [41.03325653076172, 8.655304554849863e-05, 0.9944688081741333, 0.08155885338783264], d_loss: 0.3188776969909668\n",
      "\n",
      "niter: 35401, g_loss: [41.0874137878418, 0.005655388813465834, 0.9975541234016418, 0.08175556361675262], d_loss: 0.12650737166404724\n",
      "\n",
      "niter: 35701, g_loss: [41.059207916259766, 0.003342631971463561, 0.9970123767852783, 0.08171989023685455], d_loss: 0.04465619474649429\n",
      "\n",
      "niter: 36001, g_loss: [41.052833557128906, 0.017733575776219368, 0.9968258142471313, 0.0816691517829895], d_loss: 0.018317807465791702\n",
      "\n",
      "niter: 36301, g_loss: [41.04313659667969, 0.00016207920270971954, 0.9977845549583435, 0.08125386387109756], d_loss: 0.009740980342030525\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niter: 36601, g_loss: [40.992366790771484, 0.0024174253921955824, 0.9962610602378845, 0.081072136759758], d_loss: 0.0036863377317786217\n",
      "\n",
      "niter: 36901, g_loss: [100.95006561279297, 59.94002151489258, 0.9990114569664001, 0.0819307416677475], d_loss: 0.11881913244724274\n",
      "\n",
      "niter: 37201, g_loss: [40.94556427001953, 0.00011360425560269505, 0.9972627758979797, 0.08190831542015076], d_loss: 0.0038142516277730465\n",
      "\n",
      "niter: 37501, g_loss: [40.88492965698242, 0.012464210391044617, 0.9952144622802734, 0.08188027143478394], d_loss: 0.0008711900445632637\n",
      "\n",
      "niter: 37801, g_loss: [40.923770904541016, 0.017184989526867867, 0.9986703991889954, 0.0818023681640625], d_loss: 0.0010629184544086456\n",
      "\n",
      "niter: 38101, g_loss: [40842.14453125, 40801.20703125, 0.9976341724395752, 0.08290974795818329], d_loss: 0.25247249007225037\n",
      "\n",
      "niter: 38401, g_loss: [42.17116165161133, 0.14255905151367188, 0.9985312223434448, 0.08210189640522003], d_loss: 0.6196585893630981\n",
      "\n",
      "niter: 38701, g_loss: [41.953216552734375, 0.0020060590468347073, 0.9970413446426392, 0.08195442706346512], d_loss: 0.09222034364938736\n",
      "\n",
      "niter: 39001, g_loss: [604.4926147460938, 562.5499267578125, 0.9993290305137634, 0.08183079957962036], d_loss: 0.1582856923341751\n",
      "\n",
      "niter: 39301, g_loss: [41.87067413330078, 0.002755928784608841, 0.9986342191696167, 0.08223199844360352], d_loss: 0.012494521215558052\n",
      "\n",
      "niter: 39601, g_loss: [41.80860900878906, 0.041067592799663544, 0.9970551133155823, 0.08218979835510254], d_loss: 0.004723826423287392\n",
      "\n",
      "niter: 39901, g_loss: [41.705177307128906, 0.008390693925321102, 0.997403085231781, 0.08215688169002533], d_loss: 0.0021377955563366413\n",
      "\n",
      "niter: 40201, g_loss: [47.22211837768555, 5.580594539642334, 0.9990333914756775, 0.08221554011106491], d_loss: 0.0032404912635684013\n",
      "\n",
      "niter: 40501, g_loss: [41.638916015625, 0.11629682034254074, 0.9980606436729431, 0.08222474902868271], d_loss: 0.00044560543028637767\n",
      "\n",
      "niter: 40801, g_loss: [41.69981002807617, 0.3027864098548889, 0.9973981380462646, 0.0821869820356369], d_loss: 0.0003544962964951992\n",
      "\n",
      "niter: 41101, g_loss: [42.08399200439453, 0.81387859582901, 0.9973800182342529, 0.0821840912103653], d_loss: 0.000228644727030769\n",
      "\n",
      "niter: 41401, g_loss: [41.80718231201172, 0.6934729814529419, 0.9966961145401001, 0.08213809132575989], d_loss: 0.00016402812616433948\n",
      "\n",
      "niter: 41701, g_loss: [43.84764862060547, 2.898613452911377, 0.9964704513549805, 0.08211709558963776], d_loss: 0.0007538109784945846\n",
      "\n",
      "niter: 42001, g_loss: [41.401485443115234, 0.6415985822677612, 0.9959917068481445, 0.0820847600698471], d_loss: 0.00013024805230088532\n",
      "\n",
      "niter: 42301, g_loss: [40.60462951660156, 0.042028699070215225, 0.9961593747138977, 0.08206711709499359], d_loss: 0.000422034616349265\n",
      "\n",
      "niter: 42601, g_loss: [40.828758239746094, 0.5080172419548035, 0.9952638745307922, 0.08200766891241074], d_loss: 0.00015781530237291008\n",
      "\n",
      "niter: 42901, g_loss: [40.144500732421875, 0.07442665100097656, 0.9951858520507812, 0.08194607496261597], d_loss: 0.0004861167399212718\n",
      "\n",
      "niter: 43201, g_loss: [40.50252914428711, 0.7201586961746216, 0.9945977926254272, 0.082057423889637], d_loss: 0.00010992196621373296\n",
      "\n",
      "niter: 43501, g_loss: [39.78118133544922, 0.2718852460384369, 0.9961983561515808, 0.08221834152936935], d_loss: 0.00012619032349903136\n",
      "\n",
      "niter: 43801, g_loss: [39.871726989746094, 0.7002619504928589, 0.9961094856262207, 0.08219467103481293], d_loss: 9.774899808689952e-05\n",
      "\n",
      "niter: 44101, g_loss: [44.79601287841797, 0.0007008700631558895, 0.9965850710868835, 0.07890447974205017], d_loss: 0.0019984350074082613\n",
      "\n",
      "niter: 44401, g_loss: [44.66039276123047, 0.0428643561899662, 0.9965237379074097, 0.07892622798681259], d_loss: 0.0006567729287780821\n",
      "\n",
      "niter: 44701, g_loss: [44.650428771972656, 0.22979095578193665, 0.9964357018470764, 0.07877212762832642], d_loss: 0.000321588187944144\n",
      "\n",
      "niter: 45001, g_loss: [44.498104095458984, 0.29112857580184937, 0.9965028166770935, 0.07871346175670624], d_loss: 0.00019781503942795098\n",
      "\n",
      "niter: 45301, g_loss: [44.47901153564453, 0.5118823647499084, 0.9963223934173584, 0.07885295152664185], d_loss: 0.0001427682291250676\n",
      "\n",
      "niter: 45601, g_loss: [44.177242279052734, 0.4743419289588928, 0.9960687160491943, 0.07890976220369339], d_loss: 0.00012020411668345332\n",
      "\n",
      "niter: 45901, g_loss: [43.4766960144043, 0.0010584222618490458, 0.9988953471183777, 0.07836518436670303], d_loss: 0.00011995354725513607\n",
      "\n",
      "niter: 46201, g_loss: [49.29893493652344, 0.0897257998585701, 0.9996693134307861, 0.06724093109369278], d_loss: 0.8581438064575195\n",
      "\n",
      "niter: 46501, g_loss: [53.33199691772461, 4.1978068351745605, 0.9999998211860657, 0.06722301244735718], d_loss: 1.0494354963302612\n",
      "\n",
      "niter: 46801, g_loss: [49.12174606323242, 0.07720696926116943, 0.9999998211860657, 0.06722263246774673], d_loss: 0.17872658371925354\n",
      "\n",
      "niter: 47101, g_loss: [48.95590591430664, 0.009533380158245564, 0.9999998211860657, 0.06722274422645569], d_loss: 0.0966477021574974\n",
      "\n",
      "niter: 47401, g_loss: [48.93921661376953, 0.10051988065242767, 0.9999998211860657, 0.06722293794155121], d_loss: 0.056851401925086975\n",
      "\n",
      "niter: 47701, g_loss: [48.977848052978516, 0.25722336769104004, 0.9999996423721313, 0.06722148507833481], d_loss: 0.035161715000867844\n",
      "\n",
      "niter: 48001, g_loss: [48.83171463012695, 0.24084864556789398, 0.999998927116394, 0.0672212690114975], d_loss: 0.022785404697060585\n",
      "\n",
      "niter: 48301, g_loss: [48.776126861572266, 0.3266311287879944, 0.9999992847442627, 0.0672224760055542], d_loss: 0.015464821830391884\n",
      "\n",
      "niter: 48601, g_loss: [48.636207580566406, 0.3429597318172455, 0.9999434947967529, 0.06734214723110199], d_loss: 0.010931389406323433\n",
      "\n",
      "niter: 48901, g_loss: [48.457767486572266, 0.33330297470092773, 0.9999998807907104, 0.06734751164913177], d_loss: 0.008017738349735737\n",
      "\n",
      "niter: 49201, g_loss: [48.05083084106445, 0.11419478803873062, 0.9994058609008789, 0.06712470948696136], d_loss: 0.006052893586456776\n",
      "\n",
      "niter: 49501, g_loss: [48.23005676269531, 0.49223199486732483, 0.9997002482414246, 0.0670740008354187], d_loss: 0.0045819212682545185\n",
      "\n",
      "niter: 49801, g_loss: [48.15812683105469, 0.6400793790817261, 0.9997467398643494, 0.06696386635303497], d_loss: 0.0035143964923918247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(niter):\n",
    "    \n",
    "    ### get batch x, y ###\n",
    "    x, y = train_data_generator.__next__() \n",
    "    ### train disciminator ###\n",
    "    d.trainable = True\n",
    "        \n",
    "    fake_x = g.predict(x)\n",
    "        \n",
    "    d_x = np.concatenate([x, fake_x], axis=0)\n",
    "    d_y = np.concatenate([np.zeros(len(x)), np.ones(len(fake_x))], axis=0)\n",
    "        \n",
    "    d_loss = d.train_on_batch(d_x, d_y)\n",
    "\n",
    "    ### train generator ###\n",
    "    \n",
    "    d.trainable = False        \n",
    "    g_loss = gan_trainer.train_on_batch(x, y)\n",
    "    \n",
    "    if i % 300 == 0:\n",
    "        Evaluation(dataD_test,labelD_test,i)\n",
    "        print(f'niter: {i+1}, g_loss: {g_loss}, d_loss: {d_loss}\\n')\n",
    "#         print(f'precision: {precision}, recall: {recall}, f1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = g_e.predict(dataD_test)\n",
    "gan_x = g.predict(dataD_test)\n",
    "encoded_gan = g_e.predict(gan_x)\n",
    "score = np.sum(np.absolute(encoded - encoded_gan), axis=-1)\n",
    "score = (score - np.min(score)) / (np.max(score) - np.min(score)) # map to 0~1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    D_L = np.empty([score.shape[0], ])\n",
    "    for i in range(score.shape[0]):\n",
    "    #     D_L[i] = np.mean(encoded_gan[i, :])\n",
    "        if score[i]>0.459:\n",
    "            # true/negative\n",
    "            D_L[i] = 0\n",
    "        else:\n",
    "            # false/positive\n",
    "            D_L[i] = 1\n",
    "    # fpr, tpr, thresholds = metrics.roc_curve(labelAF, D_L, pos_label = 0)\n",
    "    # Auc = metrics.auc(fpr, tpr)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labelD_test, D_L, average='binary')        \n",
    "    precision, recall, f1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x251736b40c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = 14, 5\n",
    "plt.scatter(range(len(score)),score, c=['skyblue' if x == 0 else 'pink' for x in labelD_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
